"""
–û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∞–∫—Ü–∏–π
"""
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
import logging
from datetime import datetime
from pathlib import Path
import json

try:
    from database import ChemicalDatabase
    from neural_network import NeuralNetworkModel
    from config import MODEL_CONFIG, REACTION_TYPES
except ImportError:
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    class ChemicalDatabase:
        def __init__(self, db_path=None):
            pass
        def register_user(self, *args, **kwargs): return True
        def log_action(self, *args, **kwargs): return True
        def save_experiment(self, *args, **kwargs): return 1
        def save_reaction(self, *args, **kwargs): return 1
        def get_user_stats(self, *args, **kwargs): return {}
        def get_top_users(self, *args, **kwargs): return []
        def get_experiments(self, *args, **kwargs): return []
        def get_reactions(self, *args, **kwargs): return []

    class NeuralNetworkModel:
        def __init__(self, model_type='perceptron'):
            self.model_type = model_type
            self.model = None
            self.is_trained = False
            self.scaler = None
            self.n_features_ = 20
        def create_model(self, **kwargs): pass
        def train(self, X, y): return {'status': 'success'}
        def evaluate(self, X, y): return {'status': 'success', 'accuracy': 0.95}
        def predict(self, X): return np.array([0])
        def predict_proba(self, X): return np.array([[1.0, 0.0, 0.0, 0.0]])

    MODEL_CONFIG = {
        'perceptron': {'hidden_layer_sizes': (), 'max_iter': 1000},
        'mlp': {'hidden_layer_sizes': (128, 64), 'max_iter': 3000}
    }

    REACTION_TYPES = {
        'type1': {'name': '–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è A ‚Üí B ‚Üí C ‚Üí D'},
        'type2': {'name': '–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è A ‚Üí B –∏ A ‚Üí C ‚Üí D'}
    }

logger = logging.getLogger(__name__)


class ReactionSimulator:
    """–°–∏–º—É–ª—è—Ç–æ—Ä —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∞–∫—Ü–∏–π"""

    def __init__(self):
        self.species_names = ['A', 'B', 'C', 'D']

    def extract_features(self, concentrations: Dict[str, List[float]]) -> np.ndarray:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–π –≤–µ—â–µ—Å—Ç–≤ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        features = []

        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–µ—â–µ—Å—Ç–≤–∞
        for species in self.species_names:
            conc = concentrations.get(species, [0.0])
            if len(conc) > 0:
                features.extend([
                    np.mean(conc),           # –°—Ä–µ–¥–Ω—è—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è
                    np.std(conc),            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
                    np.max(conc),            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è
                    np.min(conc),            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è
                    conc[-1] / (conc[0] + 1e-10) if conc[0] != 0 else 0,  # –û—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ–Ω–µ—á–Ω–æ–π –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π
                ])
            else:
                features.extend([0.0, 0.0, 0.0, 0.0, 0.0])

        # –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –≤–µ—â–µ—Å—Ç–≤–∞–º–∏
        for i in range(len(self.species_names)):
            for j in range(i+1, len(self.species_names)):
                s1 = self.species_names[i]
                s2 = self.species_names[j]
                c1 = concentrations.get(s1, [0.0])
                c2 = concentrations.get(s2, [0.0])

                if len(c1) > 0 and len(c2) > 0 and np.mean(c2) != 0:
                    ratio = np.mean(c1) / (np.mean(c2) + 1e-10)
                    features.append(ratio)
                else:
                    features.append(0.0)

        # –î–û–ë–ê–í–õ–ï–ù–´ –ö–õ–Æ–ß–ï–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò
        a_vals = concentrations.get('A', [0.0])
        b_vals = concentrations.get('B', [0.0])
        c_vals = concentrations.get('C', [0.0])
        d_vals = concentrations.get('D', [0.0])

        if len(a_vals) > 1:
            # 1. –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–∞–∫—Å–∏–º—É–º–æ–≤ B –∏ C
            if len(b_vals) > 0:
                b_max_idx = np.argmax(b_vals)
                b_max_norm = b_max_idx / (len(b_vals) - 1) if len(b_vals) > 1 else 0
                features.append(b_max_norm)
            else:
                features.append(0.0)

            if len(c_vals) > 0:
                c_max_idx = np.argmax(c_vals)
                c_max_norm = c_max_idx / (len(c_vals) - 1) if len(c_vals) > 1 else 0
                features.append(c_max_norm)
            else:
                features.append(0.0)

            # 2. –†–∞–∑–Ω–∏—Ü–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –º–∞–∫—Å–∏–º—É–º–æ–≤ (–∫–ª—é—á–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫)
            if len(b_vals) > 0 and len(c_vals) > 0:
                time_diff = abs(b_max_norm - c_max_norm)
                features.append(time_diff)

                # 3. –ü–æ—Ä—è–¥–æ–∫ –º–∞–∫—Å–∏–º—É–º–æ–≤ (1 –µ—Å–ª–∏ B —Ä–∞–Ω—å—à–µ C, -1 –µ—Å–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç)
                order = 1 if b_max_norm < c_max_norm else -1 if b_max_norm > c_max_norm else 0
                features.append(float(order))
            else:
                features.extend([0.0, 0.0])

            # 4. –û—Ç–Ω–æ—à–µ–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º–æ–≤ B –∏ C
            b_max = max(b_vals) if b_vals else 0
            c_max = max(c_vals) if c_vals else 0
            if c_max > 0:
                features.append(b_max / c_max)
            else:
                features.append(0.0)

            # 5. –ü–ª–æ—â–∞–¥–∏ –ø–æ–¥ –∫—Ä–∏–≤—ã–º–∏ B –∏ C (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è)
            if len(b_vals) > 1:
                b_area = np.trapz(b_vals, np.linspace(0, 1, len(b_vals)))
                features.append(b_area)
            else:
                features.append(0.0)

            if len(c_vals) > 1:
                c_area = np.trapz(c_vals, np.linspace(0, 1, len(c_vals)))
                features.append(c_area)
            else:
                features.append(0.0)

        target_features = 20
        if len(features) > target_features:
            features = features[:target_features]  # –û–±—Ä–µ–∑–∞–µ–º –ª–∏—à–Ω–∏–µ
        elif len(features) < target_features:
            features.extend([0.0] * (target_features - len(features)))  # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏

        return np.array(features)

    def analyze_reaction_pattern(self, concentrations: Dict[str, List[float]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Ä–µ–∞–∫—Ü–∏–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ—á–µ–∫"""
        a_vals = concentrations.get('A', [0.0])
        b_vals = concentrations.get('B', [0.0])
        c_vals = concentrations.get('C', [0.0])
        d_vals = concentrations.get('D', [0.0])

        if len(a_vals) < 2:
            return {'type': 'unknown', 'confidence': 0.5}

        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è: A —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è, B –ø–æ—è–≤–ª—è–µ—Ç—Å—è –∏ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è, C –ø–æ—è–≤–ª—è–µ—Ç—Å—è, D —Ä–∞—Å—Ç–µ—Ç
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è: A —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è, B –∏ C –ø–æ—è–≤–ª—è—é—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, D —Ä–∞—Å—Ç–µ—Ç –±—ã—Å—Ç—Ä–µ–µ

        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π
        a_decrease = a_vals[0] - a_vals[-1]
        b_max_idx = np.argmax(b_vals) if len(b_vals) > 0 else 0
        c_max_idx = np.argmax(c_vals) if len(c_vals) > 0 else 0
        d_increase = d_vals[-1] - d_vals[0] if len(d_vals) > 1 else 0

        # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (0-1)
        n_points = len(a_vals)
        if n_points > 1:
            b_max_norm = b_max_idx / (n_points - 1)  # 0 –¥–æ 1
            c_max_norm = c_max_idx / (n_points - 1)  # 0 –¥–æ 1
            time_diff_norm = abs(b_max_norm - c_max_norm)
        else:
            b_max_norm = 0
            c_max_norm = 0
            time_diff_norm = 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–∞–∫—Å–∏–º—É–º–æ–≤ –° –£–ß–ï–¢–û–ú –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò
        is_sequential = False
        is_branched = False

        # –ö—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π: B –∑–∞–º–µ—Ç–Ω–æ —Ä–∞–Ω—å—à–µ C (>20% –æ—Ç –æ–±—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)
        if (b_max_idx > 0 and c_max_idx > b_max_idx and d_increase > 0 and
            (c_max_norm - b_max_norm) > 0.2):  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ø–æ—Ä–æ–≥
            is_sequential = True

        # –ö—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π: B –∏ C –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (<15% —Ä–∞–∑–Ω–∏—Ü—ã)
        if (b_max_idx > 0 and c_max_idx > 0 and d_increase > 0 and
            time_diff_norm < 0.15):  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
            is_branched = True

        # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
        if is_sequential and not is_branched:
            confidence = min(0.9, 0.7 + (c_max_norm - b_max_norm) * 1.5)
            return {
                'type': 'type1',
                'confidence': confidence,
                'b_max_norm': b_max_norm,
                'c_max_norm': c_max_norm,
                'time_diff': time_diff_norm
            }
        elif is_branched and not is_sequential:
            confidence = min(0.9, 0.7 + (0.15 - time_diff_norm) * 3)
            return {
                'type': 'type2',
                'confidence': confidence,
                'b_max_norm': b_max_norm,
                'c_max_norm': c_max_norm,
                'time_diff': time_diff_norm
            }
        else:
            # –ï—Å–ª–∏ –æ–±–∞ –∏–ª–∏ –Ω–∏ –æ–¥–∏–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
            b_max = max(b_vals) if b_vals else 0
            c_max = max(c_vals) if c_vals else 0

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ä–µ–∞–∫—Ü–∏–∏: B –∏ C –∏–º–µ—é—Ç —Å—Ä–∞–≤–Ω–∏–º—ã–µ –º–∞–∫—Å–∏–º—É–º—ã
            if b_max > 0.1 and c_max > 0.1 and abs(b_max - c_max) < max(b_max, c_max) * 0.3:
                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–Ω–µ–µ 30%
                confidence = 0.7 if time_diff_norm < 0.25 else 0.6
                return {
                    'type': 'type2',
                    'confidence': confidence,
                    'b_max_norm': b_max_norm,
                    'c_max_norm': c_max_norm,
                    'time_diff': time_diff_norm
                }
            else:
                # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                confidence = 0.7 if b_max_norm < c_max_norm else 0.6
                return {
                    'type': 'type1',
                    'confidence': confidence,
                    'b_max_norm': b_max_norm,
                    'c_max_norm': c_max_norm,
                    'time_diff': time_diff_norm
                }


class ReactionBot:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –±–æ—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–π —Å–∏—Å—Ç–µ–º–æ–π"""

    def __init__(self):
        self.db = ChemicalDatabase()
        self.simulator = ReactionSimulator()
        self.current_model = None
        self.current_experiment_id = None

    def train_model(self, model_type: str = 'perceptron',
                   n_samples: int = 2000,
                   max_iter: int = 3000,
                   hidden_layers: tuple = None) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Å–µ–º–µ–Ω–∏"""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.current_model = NeuralNetworkModel(model_type)

            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ MODEL_CONFIG
            config = MODEL_CONFIG.get(model_type, MODEL_CONFIG.get('perceptron', {})).copy()
            config['max_iter'] = max_iter

            if model_type == 'mlp' and hidden_layers:
                config['hidden_layer_sizes'] = hidden_layers

            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            if hasattr(self.current_model, 'create_model'):
                # –£–±–∏—Ä–∞–µ–º random_state –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                config.pop('random_state', None)
                self.current_model.create_model(**config)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—É—á–∞–µ–º
            X, y = self.current_model.generate_training_data(n_samples)

            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2
            )

            # –û–±—É—á–µ–Ω–∏–µ
            train_result = self.current_model.train(X_train, y_train, X_test, y_test)
            if isinstance(train_result, dict) and train_result.get('status') == 'error':
                return train_result

            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
            eval_result = self.current_model.evaluate(X_test, y_test)

            if isinstance(eval_result, dict) and eval_result.get('status') == 'success':
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                experiment_data = {
                    'experiment_name': f'–û–±—É—á–µ–Ω–∏–µ {model_type}',
                    'model_type': model_type,
                    'accuracy': eval_result.get('accuracy', 0),
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'parameters': {
                        'n_samples': n_samples,
                        'max_iter': max_iter,
                        'model_type': model_type,
                        'hidden_layers': hidden_layers,
                        'n_features': self.current_model.n_features_
                    }
                }

                self.current_experiment_id = self.db.save_experiment(0, experiment_data)

                return {
                    'status': 'success',
                    'accuracy': eval_result.get('accuracy', 0),
                    'experiment_id': self.current_experiment_id,
                    'model_type': model_type,
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'n_iterations': getattr(self.current_model.model, 'n_iter_', 0)
                    if hasattr(self.current_model, 'model') else 0,
                    'n_features': self.current_model.n_features_
                }
            else:
                return eval_result if isinstance(eval_result, dict) else {
                    'status': 'error',
                    'message': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ'
                }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
            return {'status': 'error', 'message': str(e)}

    def predict_reaction(self, time_points: List[float],
                         concentrations: Dict[str, List[float]],
                         user_id: int = 0) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞ —Ä–µ–∞–∫—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –ë–î –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏"""
        try:
            # ================== –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ==================
            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏
            logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}...")

            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–π –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            processed_concentrations = {}
            for substance, values in concentrations.items():
                if not values:
                    processed_concentrations[substance] = [0.0] * len(time_points)
                    logger.warning(f"–í–µ—â–µ—Å—Ç–≤–æ {substance}: –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤, –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –Ω—É–ª—è–º–∏")
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ NaN
                clean_values = []
                negative_count = 0
                nan_count = 0

                for v in values:
                    if np.isnan(v) or v is None:
                        clean_values.append(0.0)
                        nan_count += 1
                    elif v < 0:
                        clean_values.append(0.0)
                        negative_count += 1
                    else:
                        clean_values.append(float(v))

                processed_concentrations[substance] = clean_values

                if negative_count > 0:
                    logger.warning(f"–í–µ—â–µ—Å—Ç–≤–æ {substance}: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {negative_count} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
                if nan_count > 0:
                    logger.warning(f"–í–µ—â–µ—Å—Ç–≤–æ {substance}: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {nan_count} NaN/None –∑–Ω–∞—á–µ–Ω–∏–π")

            # 2. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –≤–µ—â–µ—Å—Ç–≤ (A, B, C, D)
            required_substances = ['A', 'B', 'C', 'D']
            for substance in required_substances:
                if substance not in processed_concentrations:
                    processed_concentrations[substance] = [0.0] * len(time_points)
                    logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–µ –≤–µ—â–µ—Å—Ç–≤–æ {substance}")

            # 3. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
            n_points = len(time_points)
            normalized_concentrations = {s: [] for s in required_substances}

            for i in range(n_points):
                # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
                current_values = {}
                for s in required_substances:
                    if i < len(processed_concentrations[s]):
                        current_values[s] = processed_concentrations[s][i]
                    else:
                        current_values[s] = 0.0

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é —Å—É–º–º—É
                total = sum(current_values.values())

                if total <= 0:
                    current_values = {
                        'A': max(0, 1.0 - 0.1 * i),
                        'B': max(0, 0.0 + 0.08 * i),
                        'C': max(0, 0.0 + 0.05 * i),
                        'D': max(0, 0.0 + 0.03 * i)
                    }
                    total = sum(current_values.values())
                    logger.warning(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞ {i}: –≤—Å–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –Ω—É–ª–µ–≤—ã–µ, —Å–æ–∑–¥–∞–Ω—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")

                for s in required_substances:
                    normalized_concentrations[s].append(current_values[s])

            # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—ã –º–∞—Å—Å–∏–≤–æ–≤
            lengths = [len(v) for v in normalized_concentrations.values()]
            if len(set(lengths)) > 1:
                logger.error(f"–†–∞–∑–Ω—ã–µ –¥–ª–∏–Ω—ã –º–∞—Å—Å–∏–≤–æ–≤ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {lengths}")
                min_len = min(lengths)
                for s in required_substances:
                    normalized_concentrations[s] = normalized_concentrations[s][:min_len]
                time_points = time_points[:min_len]

            # ================== –ê–ù–ê–õ–ò–ó –†–ï–ê–ö–¶–ò–ò ==================
            logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ä–µ–∞–∫—Ü–∏–∏ —Å {len(time_points)} –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–æ—á–∫–∞–º–∏...")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
            pattern_analysis = self.simulator.analyze_reaction_pattern(normalized_concentrations)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–ª–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫—É
            if not self.current_model or not hasattr(self.current_model,
                                                     'is_trained') or not self.current_model.is_trained:
                logger.warning("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
                result = {
                    'status': 'success',
                    'reaction_type': pattern_analysis['type'],
                    'confidence': pattern_analysis['confidence'],
                    'type_name': REACTION_TYPES.get(pattern_analysis['type'], {}).get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø'),
                    'method': 'heuristic',
                    'data_quality': 'processed',
                    'analysis_details': {
                        'points_analyzed': len(time_points),
                        'b_max_norm': pattern_analysis.get('b_max_norm', 0),
                        'c_max_norm': pattern_analysis.get('c_max_norm', 0),
                        'time_diff': pattern_analysis.get('time_diff', 0)
                    }
                }
            else:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                features = self.simulator.extract_features(normalized_concentrations).reshape(1, -1)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                if hasattr(self.current_model, 'n_features_'):
                    n_features_expected = self.current_model.n_features_
                    n_features_actual = features.shape[1]

                    if n_features_actual != n_features_expected:
                        logger.info(f"–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {n_features_actual} -> {n_features_expected}")

                        if n_features_actual > n_features_expected:
                            features = features[:, :n_features_expected]
                        else:
                            padding = np.zeros((1, n_features_expected - n_features_actual))
                            features = np.hstack([features, padding])

                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                try:
                    prediction = self.current_model.predict(features)
                    probability = self.current_model.predict_proba(features)

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–µ–∞–∫—Ü–∏–∏
                    reaction_code = f'type{prediction[0] + 1}'  # +1 —Ç.–∫. –∫–ª–∞—Å—Å—ã 0 –∏ 1
                    confidence = float(probability[0][prediction[0]])

                    type_info = REACTION_TYPES.get(reaction_code, REACTION_TYPES.get('type1', {}))
                    type_name = type_info.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø')

                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —ç–≤—Ä–∏—Å—Ç–∏–∫–æ–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                    # –ï–°–õ–ò –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–í–ü–ê–î–ê–Æ–¢ - –ø–æ–≤—ã—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    if pattern_analysis['type'] == reaction_code:
                        combined_confidence = max(confidence, pattern_analysis['confidence'])
                        # –ï—Å–ª–∏ –æ–±–∞ –º–µ—Ç–æ–¥–∞ —É–≤–µ—Ä–µ–Ω—ã - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å
                        if confidence > 0.7 and pattern_analysis['confidence'] > 0.7:
                            combined_confidence = min(1.0, combined_confidence + 0.05)
                        confidence = combined_confidence
                        method = 'combined_agreement'
                    else:
                        # –ï—Å–ª–∏ –º–µ—Ç–æ–¥—ã —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
                        # –ë–æ–ª—å—à–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–∞
                        model_weight = 0.7 if confidence > 0.8 else 0.6
                        heuristic_weight = 1 - model_weight
                        confidence = (confidence * model_weight +
                                      pattern_analysis['confidence'] * heuristic_weight)
                        method = 'weighted_disagreement'

                        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                        logger.info(f"–ú–µ—Ç–æ–¥—ã —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è: –º–æ–¥–µ–ª—å={reaction_code}({confidence:.2f}), "
                                  f"—ç–≤—Ä–∏—Å—Ç–∏–∫–∞={pattern_analysis['type']}({pattern_analysis['confidence']:.2f})")

                    result = {
                        'status': 'success',
                        'reaction_type': reaction_code,
                        'confidence': confidence,
                        'type_name': type_name,
                        'method': method,
                        'data_quality': 'processed',
                        'analysis_details': {
                            'model_prediction': reaction_code,
                            'heuristic_prediction': pattern_analysis['type'],
                            'model_confidence': float(probability[0][prediction[0]]),
                            'heuristic_confidence': pattern_analysis['confidence'],
                            'points_analyzed': len(time_points),
                            'b_max_norm': pattern_analysis.get('b_max_norm', 0),
                            'c_max_norm': pattern_analysis.get('c_max_norm', 0),
                            'time_diff': pattern_analysis.get('time_diff', 0)
                        }
                    }

                except Exception as model_error:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª—å—é: {model_error}")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
                    result = {
                        'status': 'success',
                        'reaction_type': pattern_analysis['type'],
                        'confidence': pattern_analysis['confidence'] * 0.9,  # –ß—É—Ç—å —É–º–µ–Ω—å—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                        'type_name': REACTION_TYPES.get(pattern_analysis['type'], {}).get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø'),
                        'method': 'heuristic_fallback',
                        'data_quality': 'processed',
                        'model_error': str(model_error),
                        'analysis_details': {
                            'fallback_reason': 'model_error',
                            'heuristic_confidence': pattern_analysis['confidence'],
                            'points_analyzed': len(time_points),
                            'b_max_norm': pattern_analysis.get('b_max_norm', 0),
                            'c_max_norm': pattern_analysis.get('c_max_norm', 0),
                            'time_diff': pattern_analysis.get('time_diff', 0)
                        }
                    }

            # ================== –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–ê–ó–£ –î–ê–ù–ù–´–• ==================
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∞–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}...")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            reaction_data = {
                'reaction_type': result.get('reaction_type', 'unknown'),
                'substances': list(normalized_concentrations.keys()),
                'concentrations': normalized_concentrations,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                'time_points': time_points,
                'prediction_result': result,
                'confidence': result.get('confidence', 0.0),
                'original_data_quality': {
                    'has_negatives': any(v < 0 for values in concentrations.values() for v in values),
                    'has_nan': any(np.isnan(v) for values in concentrations.values() for v in values),
                    'num_points': len(time_points)
                }
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î —Å —Ä–µ–∞–ª—å–Ω—ã–º user_id
            try:
                reaction_id = self.db.save_reaction(user_id, reaction_data)

                if reaction_id > 0:
                    result['reaction_id'] = reaction_id
                    result['db_status'] = 'success'
                    logger.info(f"‚úÖ –†–µ–∞–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ë–î —Å ID: {reaction_id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    result['data_processing_info'] = {
                        'points_processed': len(time_points),
                        'substances_analyzed': len(normalized_concentrations),
                        'original_data_points': sum(len(v) for v in concentrations.values()),
                        'data_corrections_applied': result.get('data_quality') == 'processed'
                    }
                else:
                    result['db_status'] = 'failed'
                    result['db_error'] = '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å ID —Ä–µ–∞–∫—Ü–∏–∏'
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å ID —Ä–µ–∞–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

            except Exception as db_error:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {db_error}")
                result['db_status'] = 'error'
                result['db_error'] = str(db_error)
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î

            # ================== –í–ê–õ–ò–î–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–ê ==================
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
            confidence = result.get('confidence', 0)
            if confidence < 0:
                result['confidence'] = 0.0
                logger.warning("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
            elif confidence > 1:
                result['confidence'] = min(1.0, confidence)
                logger.warning("–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å > 1")

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if confidence > 0.8:
                result['quality'] = 'high'
                quality_emoji = 'üîµ'
            elif confidence > 0.6:
                result['quality'] = 'medium'
                quality_emoji = 'üü°'
            else:
                result['quality'] = 'low'
                quality_emoji = 'üî¥'
                logger.warning(f"–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {confidence:.2%}")

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞
            result['analysis_timestamp'] = datetime.now().isoformat()
            result['quality_emoji'] = quality_emoji

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è —Ç–∏–ø–∞ —Ä–µ–∞–∫—Ü–∏–∏
            if result.get('reaction_type') == 'type1':
                result['reaction_emoji'] = '‚û°Ô∏è'
            elif result.get('reaction_type') == 'type2':
                result['reaction_emoji'] = 'üå≥'
            else:
                result['reaction_emoji'] = '‚ùì'

            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            logger.info(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {result.get('type_name')}, "
                       f"—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}, –∫–∞—á–µ—Å—Ç–≤–æ: {result.get('quality')}")

            return result

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}", exc_info=True)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
            return {
                'status': 'error',
                'message': f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}",
                'error_type': type(e).__name__,
                'user_id': user_id,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'suggestion': '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞'
            }

    def get_experiments(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
        try:
            return self.db.get_experiments()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def get_predictions(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        try:
            return self.db.get_reactions()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π: {e}")
            return []

    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            experiments = self.db.get_experiments()
            predictions = self.db.get_reactions()

            total_experiments = len(experiments)
            total_predictions = len(predictions)

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —Ç–æ—á–Ω–æ—Å—Ç—å
            accuracies = [exp.get('accuracy', 0) for exp in experiments if exp.get('accuracy')]
            average_accuracy = sum(accuracies) / len(accuracies) if accuracies else 0

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidences = [pred.get('confidence', 0) for pred in predictions if pred.get('confidence')]
            average_confidence = sum(confidences) / len(confidences) if confidences else 0

            return {
                'total_experiments': total_experiments,
                'total_predictions': total_predictions,
                'average_accuracy': average_accuracy,
                'average_confidence': average_confidence
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {
                'total_experiments': 0,
                'total_predictions': 0,
                'average_accuracy': 0,
                'average_confidence': 0
            }